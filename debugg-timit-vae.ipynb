{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Using Extended Kalman Filter for Speech Recognition using TIMIT  \n",
    "\n",
    "The purpose of this demo is to help you learn about variational autoencoder. The algorithm is being implemented is from the paper \"Auto-Encoding Variational Bayes\", by Diederik P Kingma, Max Welling (https://arxiv.org/abs/1312.6114).\n",
    "\n",
    "Followed the logic:\n",
    "\n",
    "VAE Basic: https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/\n",
    "\n",
    "VAE: https://github.com/ethanluoyc/pytorch-vae/blob/master/vae.py \n",
    "\n",
    "VAE: https://github.com/Baileyswu/pytorch-hmm-vae/blob/master/vae.py\n",
    "\n",
    "TIMIT: https://github.com/jackjhliu/Pytorch-End-to-End-ASR-on-TIMIT\n",
    "\n",
    "EKF: https://github.com/jnez71/kalmaNN\n",
    "\n",
    "We are using TIMIT data.\n",
    "\n",
    "You are free to change model acrhitecture, or any part of the logic. \n",
    "\n",
    "If you have any suggestions or find errors, please, don't be hesitate to text me at jayanta.jayantamukherjee@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/.conda/envs/jay/lib/python3.6/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import data\n",
    "import eval_utils\n",
    "import logging, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from prepare_data import prepare_csv\n",
    "from show_history import plotLearning\n",
    "import time \n",
    "import timeit\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data from Raw WAV files: TIMIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN.csv is created.\n",
      "DEV.csv is created.\n",
      "TEST.csv is created.\n",
      "Data preparation is complete !\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(stream=sys.stderr, level=logging.INFO)\n",
    "\n",
    "prepare_csv(\"../TIMIT/TIMIT_DATA/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Config & Clean up Previous Run Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgFile = \"config/default.yaml\" \n",
    "import shutil\n",
    "\n",
    "with open(cfgFile) as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        \n",
    "if not cfg['logdir']:\n",
    "    save_path = os.path.splitext(cfgFile)[0]\n",
    "    \n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "workers = 0\n",
    "ckpt_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 240 # 240 is the dimension of acoustic features.\n",
    "# define a simple linear VAE\n",
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self, target_size, hidden_size, encoder_layers, decoder_layers, drop_p=0.):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target_size (integer): Target vocabulary size.\n",
    "            hidden_size (integer): Size of GRU cells.\n",
    "            encoder_layers (integer): EncoderRNN layers.\n",
    "            decoder_layers (integer): DecoderRNN layers.\n",
    "            drop_p (float): Probability to drop elements at Dropout layers.\n",
    "        \"\"\"\n",
    "        super(LinearVAE, self).__init__()\n",
    "        print(\"Init LinearVAE\")\n",
    "\n",
    "        #self.encoder = EncoderRNN(hidden_size, encoder_layers, drop_p)\n",
    "        #self.decoder = DecoderRNN(target_size, hidden_size, decoder_layers, drop_p)\n",
    "         \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=features, out_features=512)\n",
    "        self.enc2 = nn.Linear(in_features=512, out_features=hidden_size)\n",
    "        self.enc22 = nn.Linear(in_features=512, out_features=hidden_size) \n",
    " \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=hidden_size, out_features=512)\n",
    "        self.dec2 = nn.Linear(in_features=512, out_features=features)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.enc1(x))\n",
    "        return self.enc2(h1), self.enc22(h1) \n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.dec1(z))\n",
    "        return torch.sigmoid(self.dec2(h3))\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "    \n",
    "    def forward(self, xs, xlens, ys=None):\n",
    "        \"\"\"\n",
    "        The forwarding behavior depends on if ground-truths are provided.\n",
    "\n",
    "        Args:\n",
    "            xs (torch.LongTensor, [batch_size, seq_length, dim_features]): A mini-batch of FBANK features.\n",
    "            xlens (torch.LongTensor, [batch_size]): Sequence lengths before padding.\n",
    "            ys (torch.LongTensor, [batch_size, padded_length_of_target_sentences]): Padded ground-truths.\n",
    "\n",
    "        Returns: \n",
    "            predictions (torch.FloatTensor, [batch_size, max_length]): The sentence generated by Greedy Search. \n",
    "        \"\"\" \n",
    "        # encoding\n",
    "        mu, log_var = self.encode(xs)\n",
    "        print(\"mu shape = \", mu.shape)\n",
    "        print(\"log_var shape = \", log_var.shape)\n",
    "        \n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        print(\"4 z shape = \", z.shape)\n",
    "        \n",
    "        reconstruction = self.decode(z)\n",
    "        print(\"6 reconstruction shape = \", reconstruction.shape)\n",
    "        \n",
    "        return reconstruction, mu, log_var\n",
    "  \n",
    "            \n",
    "    def get_lr(self, optimizer):\n",
    "        \"\"\"\n",
    "        A helper function to retrieve the solver's learning rate.\n",
    "        \"\"\"\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "\n",
    "    def log_history(self, save_path, message):\n",
    "        \"\"\"\n",
    "        A helper function to log the history.\n",
    "        The history text file is saved as: {SAVE_PATH}/history.txt\n",
    "\n",
    "        Args:\n",
    "            save_path (string): The location to log the history.\n",
    "            message (string): The message to log.\n",
    "        \"\"\"\n",
    "        fname = os.path.join(save_path,'history.csv')\n",
    "        if not os.path.exists(fname):\n",
    "            with open(fname, 'w') as f:\n",
    "                f.write(\"datetime,epoch,learning rate,train loss,dev loss,error rate\\n\")\n",
    "                f.write(\"%s\\n\" % message)\n",
    "        else:\n",
    "            with open(fname, 'a') as f:\n",
    "                f.write(\"%s\\n\" % message)\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, filename, save_path, epoch, dev_error, cfg, weights):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filename (string): Filename of this checkpoint.\n",
    "            save_path (string): The location to save.\n",
    "            epoch (integer): Epoch number.\n",
    "            dev_error (float): Error rate on development set.\n",
    "            cfg (dict): Experiment config for reconstruction.\n",
    "            weights (dict): \"state_dict\" of this model.\n",
    "        \"\"\"\n",
    "        filename = os.path.join(save_path, filename)\n",
    "        info = {'epoch': epoch,\n",
    "                'dev_error': dev_error,\n",
    "                'cfg': cfg,\n",
    "                'weights': weights}\n",
    "        torch.save(info, filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Learning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leanring parameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Linear VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN set size: 3696\n",
      "DEV set size: 1152\n",
      "tokenizer.vocab length =  66\n",
      "Init LinearVAE\n"
     ]
    }
   ],
   "source": [
    "gpu_id = 0\n",
    "workers = 0\n",
    "ckpt_freq = 10\n",
    "\n",
    "#input_dim = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = cfg['train']['batch_size']\n",
    "#transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Create dataset\n",
    "train_loader = data.load(split='train', batch_size=batch_size, workers = workers)\n",
    "val_loader = data.load(split='dev', batch_size=batch_size)\n",
    "hidden_size = hidden_size=cfg['model']['hidden_size']\n",
    "activation=cfg['model']['activation']\n",
    "\n",
    "# Build model\n",
    "tokenizer = torch.load('tokenizer.pth')\n",
    "print(\"tokenizer.vocab length = \", len(tokenizer.vocab))\n",
    "#      self, target_size, hidden_size, encoder_layers, decoder_layers, drop_p=0.\n",
    "model = LinearVAE(target_size=len(tokenizer.vocab),\n",
    "          hidden_size=cfg['model']['hidden_size'],\n",
    "          encoder_layers=cfg['model']['encoder_layers'],\n",
    "          decoder_layers=cfg['model']['decoder_layers'],\n",
    "          drop_p=cfg['model']['drop_p'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Final Loss Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(dataloader.dataset)/dataloader.batch_size)):\n",
    "        (xs, xlens, ys) = data\n",
    "        \n",
    "        print(\"inside fit::: xs shape = \", xs.shape, \", xlens shape = \", xlens.shape, \", ys shape = \", ys.shape)\n",
    "        #data, _ = data\n",
    "        #data = data.to(device)\n",
    "        #data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(xs, xlens)\n",
    "        print(\"xs shape = \", xs.shape, \"ys shape = \", ys.shape, \", reconstruction shape = \", reconstruction.shape)\n",
    "        #ys1 = ys.reshape(reconstruction.shape)\n",
    "        #reconstruction = reconstruction.reshape(ys.shape)\n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "        bce_loss = criterion(reconstruction, xs)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Validation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataloader.dataset)/dataloader.batch_size)):\n",
    "            (xs, xlens, ys) = data\n",
    "            print(\"inside validate::: xs shape = \", xs.shape, \", xlens shape = \", xlens.shape, \", ys shape = \", ys.shape)\n",
    "            reconstruction, mu, logvar = model(xs, xlens)\n",
    "            print(\"xs shape = \", xs.shape, \"ys shape = \", ys.shape, \", reconstruction shape = \", reconstruction.shape)\n",
    "            #criterion = nn.CrossEntropyLoss()\n",
    "            bce_loss = criterion(reconstruction, xs)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            #if i == int(len(dataloader.dataset)/dataloader.batch_size) - 1:\n",
    "            #    num_rows = 8\n",
    "            #    both = torch.cat((data.view(batch_size, 1, 28, 28)[:8], \n",
    "            #                      reconstruction.view(batch_size, 1, 28, 28)[:8]))\n",
    "            #    save_image(both.cpu(), f\"../outputs/output{epoch}.png\", nrow=num_rows)\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2\n",
      "Learning rate: 0.000100\n",
      "table =  {'aa': 'aa', 'ae': 'ae', 'ah': 'ah', 'ao': 'aa', 'aw': 'aw', 'ax': 'ah', 'ax-h': 'ah', 'axr': 'er', 'ay': 'ay', 'b': 'b', 'bcl': 'sil', 'ch': 'ch', 'd': 'd', 'dcl': 'sil', 'dh': 'dh', 'dx': 'dx', 'eh': 'eh', 'el': 'l', 'em': 'm', 'en': 'n', 'eng': 'ng', 'epi': 'sil', 'er': 'er', 'ey': 'ey', 'f': 'f', 'g': 'g', 'gcl': 'sil', 'h#': 'sil', 'hh': 'hh', 'hv': 'hh', 'ih': 'ih', 'ix': 'ih', 'iy': 'iy', 'jh': 'jh', 'k': 'k', 'kcl': 'sil', 'l': 'l', 'm': 'm', 'n': 'n', 'ng': 'ng', 'nx': 'n', 'ow': 'ow', 'oy': 'oy', 'p': 'p', 'pau': 'sil', 'pcl': 'sil', 'r': 'r', 's': 's', 'sh': 'sh', 't': 't', 'tcl': 'sil', 'th': 'th', 'uh': 'uh', 'uw': 'uw', 'ux': 'uw', 'v': 'v', 'w': 'w', 'y': 'y', 'z': 'z', 'zh': 'sh'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/.conda/envs/jay/lib/python3.6/site-packages/torchaudio/compliance/kaldi.py:574: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  fft = torch.rfft(strided_input, 1, normalized=False, onesided=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu shape =  torch.Size([64, 174, 256])\n",
      "log_var shape =  torch.Size([64, 174, 256])\n",
      "4 z shape =  torch.Size([64, 174, 256])\n",
      "6 reconstruction shape =  torch.Size([64, 174, 240])\n",
      "preds_batch shape =  torch.Size([64, 174, 240])  ys shape =  torch.Size([64, 66])\n",
      "ys[ 0 ] =  tensor([ 5, 45, 11, 12, 11, 34,  6, 10, 11, 12, 43, 31, 21, 18, 14, 15, 13, 53,\n",
      "        12, 22, 34,  6, 20, 13, 43, 12, 11, 12, 43, 44, 32, 33, 26, 42, 29, 13,\n",
      "        29, 49, 12, 11, 51, 23, 24, 39, 12, 32, 10, 50, 30, 31, 61, 43, 44, 49,\n",
      "        18, 14, 15, 59, 26, 42, 18, 11, 43, 44,  5,  2])\n",
      "gt length =  176  gt =  h# dh ih s ih pcl p l ih s tcl k ay n dcl d ix v s eh pcl p r ix tcl s ih s tcl t ah m y ux z ix z ey s ih ng gcl g el s ah l f kcl k en tcl t ey n dcl d pau y ux n ih tcl t h#\n",
      "preds_batch[ 0  ] shape =  torch.Size([174, 240])\n",
      "oned_preds shape =  torch.Size([1, 41760])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a170192e915d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learning rate: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mssqrtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mRMS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssqrtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/myhome/jay/Documents/research/vae/debugg-timit-vae/eval_utils.py\u001b[0m in \u001b[0;36mget_error\u001b[0;34m(dataloader, model)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0moned_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oned_preds shape = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moned_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moned_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;31m# Sequences are mapped from 61 to 39 phonemes during evaluation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jay/lib/python3.6/site-packages/torchnlp/encoders/text/static_tokenizer_encoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, encoded)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[1;32m    131\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jay/lib/python3.6/site-packages/torchnlp/encoders/text/static_tokenizer_encoder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[1;32m    131\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "import eval_utils\n",
    "\n",
    "epochs = 2\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "RMS = []\n",
    "train_epoch_durations = []\n",
    "eval_epoch_durations = []\n",
    "best_epoch = 0\n",
    "best_error = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    \n",
    "    print(\"Learning rate: %f\" % lr)\n",
    "    ssqrtm = eval_utils.get_error(train_loader, model)\n",
    "    RMS.append(ssqrtm)\n",
    "    \n",
    "    \n",
    "    start_train_epoch = time.time()\n",
    "    train_epoch_loss = fit(model, train_loader)\n",
    "    end_train_epoch = time.time()\n",
    "    train_epoch_duration = end_train_epoch - start_train_epoch\n",
    "    train_epoch_durations.append(train_epoch_duration)\n",
    "\n",
    "    val_epoch_loss = validate(model, val_loader)\n",
    "    end_eval_epoch = time.time()\n",
    "    eval_epoch_duration = end_eval_epoch - end_train_epoch\n",
    "    eval_epoch_durations.append(eval_epoch_duration) \n",
    "    \n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    # Compute dev error rate\n",
    "    error = eval_utils.get_error(val_loader, model)\n",
    "    print (\"Dev. loss: %.3f,\" % val_loss, end=' ')\n",
    "    print (\"dev. error rate: %.4f\" % error)\n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_epoch = epoch\n",
    "        # Save best model\n",
    "        save_checkpoint(\"best.pth\", save_path, best_epoch, best_error, cfg, model.state_dict())\n",
    "    print (\"Best dev. error rate: %.4f @epoch: %d\" % (best_error, best_epoch))\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if not epoch%ckpt_freq or epoch==cfg['train']['epochs']:\n",
    "        save_checkpoint(\"checkpoint_%05d.pth\"%epoch, save_path, epoch, error, cfg, model.state_dict())\n",
    "\n",
    "    # Logging\n",
    "    datetime = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "    msg = \"%s,%d,%f,%f,%f,%f\" % (datetime, epoch, lr, train_loss,  val_loss, error)\n",
    "    log_history(save_path, msg)\n",
    "    \n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearning(\"config/default/history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "\n",
    "batch_size = cfg['train']['batch_size']\n",
    "input_dim = 28 * 28\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for step, (xs, xlens, ys) in enumerate(train_loader):\n",
    "    xs = xs.to(device) \n",
    "    ys = ys.to(device)\n",
    "    #inputs = Variable(data.resize_(batch_size, input_dim))\n",
    "\n",
    "modeled_data = model(xs, xlens, ys) #model(inputs)\n",
    "print(\"modeled_data (loss) = \", modeled_data)\n",
    "print(\"xs = \", xs)\n",
    "print(\"ys = \", ys)\n",
    "#model_data = modeled_data[0].reshape(batch_size, input_dim)\n",
    "#print(input_data)        \n",
    "#plt.imshow(model_data[0].detach().numpy().reshape(28, 28), cmap='gray')\n",
    "#plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore checkpoint\n",
    "info = torch.load(\"config/default/best.pth\")\n",
    "split = 'test'\n",
    "print (\"Dev. error rate of checkpoint: %.4f @epoch: %d\" % (info['dev_error'], info['epoch']))\n",
    "    \n",
    "# Build model\n",
    "tokenizer = torch.load('tokenizer.pth')\n",
    "model = KalmanVAE(input_size=cfg['model']['input_size'],\n",
    "            target_size=len(tokenizer.vocab),\n",
    "            hidden_size=cfg['model']['hidden_size'],\n",
    "            encoder_layers=cfg['model']['encoder_layers'],\n",
    "            decoder_layers=cfg['model']['decoder_layers'],\n",
    "            activation=cfg['model']['activation'],\n",
    "            drop_p=cfg['model']['drop_p'])\n",
    " \n",
    "model.load_state_dict(info['weights'])\n",
    "model.eval() \n",
    "model.cuda()\n",
    "\n",
    "# Evaluate\n",
    "error = eval_utils.get_error(train_loader, model)\n",
    "print (\"Error rate on %s set = %.4f\" % (split, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training & Eval time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that using plt.subplots below is equivalent to using\n",
    "# fig = plt.figure() and then ax = fig.add_subplot(111)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#now create y values for the second plot\n",
    "y = train_epoch_durations\n",
    "#calculate the values for the Gaussian curve\n",
    "x = np.arange(len(train_epoch_durations))\n",
    "#plot the Gaussian curve\n",
    "ax.plot(x, y, label = \"Train Time\")\n",
    "\n",
    "ax.set(xlabel='Epoch (s)', ylabel='Time (ms)',\n",
    "       title='Training time')\n",
    "\n",
    "xe = np.arange(len(eval_epoch_durations))\n",
    "ye = eval_epoch_durations\n",
    "#plot sine wave\n",
    "ax.plot(xe, ye, label = \"Evaluation Time\")\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "#show the legend\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that using plt.subplots below is equivalent to using\n",
    "# fig = plt.figure() and then ax = fig.add_subplot(111)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#now create y values for the second plot\n",
    "y = RMS\n",
    "#calculate the values for the Gaussian curve\n",
    "x = np.arange(len(RMS))\n",
    "#plot the Gaussian curve\n",
    "ax.plot(x, y, label = \"Root-mean Square Error\")\n",
    "\n",
    "ax.set(xlabel='Epoch (s)', ylabel='RMSE (%)',\n",
    "       title='Root-mean Square Error')\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "#show the legend\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('img/rmse-ekf.pdf')\n",
    "plt.savefig('img/rmse-ekf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference auxiliary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "def showAttention(predictions, attentions):\n",
    "    output_words = predictions.split()\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(10,15))\n",
    "    #figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for (x, xlens, y) in train_loader:\n",
    "        predictions, attentions = model(x.cuda(), xlens)\n",
    "        predictions, attentions = predictions[0], attentions[0]\n",
    "        predictions = tokenizer.decode(predictions)\n",
    "        attentions = attentions[:len(predictions.split())].cpu().numpy()   # (target_length, source_length)\n",
    "        ground_truth = tokenizer.decode(y[0])\n",
    "        print (\"Predict:\")\n",
    "        print (predictions)\n",
    "        print (\"Ground-truth:\")\n",
    "        print (ground_truth)\n",
    "        print ()\n",
    "        showAttention(predictions, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "with open('config/default/timing.csv', 'w') as filehandle:\n",
    "    for (train_epoch_duration, eval_epoch_duration, RMSE) in zip(train_epoch_durations, eval_epoch_durations, RMS):\n",
    "        msg = '%d, %s, %s, %s \\n' % (epoch, train_epoch_duration, eval_epoch_duration, RMSE)\n",
    "        #print(msg)\n",
    "        epoch = epoch +1 \n",
    "        filehandle.write(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
